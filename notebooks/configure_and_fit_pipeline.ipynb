{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuracion inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd())) \n",
    "data_processed_path = os.path.join(project_path, \"..\", \"data\", \"processed\")\n",
    "artifacts_path = os.path.join(project_path, \"..\", \"artifacts\")\n",
    "pipeline_path = os.path.join(artifacts_path, \"base_pipeline.pkl\")\n",
    "final_pipeline_path = os.path.join(artifacts_path, \"best_model_pipeline.pkl\")\n",
    "results_file_path = os.path.join(artifacts_path, \"model_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta de artefactos si no existe\n",
    "os.makedirs(artifacts_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Carga los datos procesados.\"\"\"\n",
    "    try:\n",
    "        print(\"\\nCargando datos procesados...\")\n",
    "        X_train = pd.read_csv(os.path.join(data_processed_path, \"X_train.csv\"))\n",
    "        y_train = pd.read_csv(os.path.join(data_processed_path, \"y_train.csv\"))\n",
    "        X_test = pd.read_csv(os.path.join(data_processed_path, \"X_test.csv\"))\n",
    "        y_test = pd.read_csv(os.path.join(data_processed_path, \"y_test.csv\"))\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error al cargar los datos procesados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar columnas del pipeline\n",
    "def validate_pipeline_columns(pipeline, X):\n",
    "    updated_steps = []\n",
    "    for name, step in pipeline.steps:\n",
    "        if hasattr(step, \"features_to_drop\"):\n",
    "            valid_features = [col for col in step.features_to_drop if col in X.columns]\n",
    "            if valid_features:\n",
    "                step.features_to_drop = valid_features\n",
    "                updated_steps.append((name, step))\n",
    "            else:\n",
    "                print(f\"El paso '{name}' fue eliminado porque no tiene columnas válidas.\")\n",
    "        else:\n",
    "            updated_steps.append((name, step))\n",
    "    return Pipeline(updated_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos e hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models():\n",
    "    \"\"\"Define los modelos a entrenar.\"\"\"\n",
    "    return {\n",
    "        \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"SVC\": SVC(random_state=42, probability=True),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric=\"logloss\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para definir los hiperparámetros de cada modelo\n",
    "def get_hyperparameter_grid(model_name):\n",
    "    \"\"\"\n",
    "    Devuelve un diccionario con el grid de hiperparámetros para cada modelo.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con los hiperparámetros.\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10]},\n",
    "        \"Decision Tree\": {\"max_depth\": [5, 10, 20, None]},\n",
    "        \"Random Forest\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]},\n",
    "        \"SVC\": {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]},\n",
    "        \"XGBoost\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 5, 7]}\n",
    "    }\n",
    "    return param_grids.get(model_name, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_hyperparameters(model_name, model, param_grid, X_train, y_train):\n",
    "    \"\"\"Entrena un modelo y ajusta los hiperparámetros si el grid está definido.\"\"\"\n",
    "    if param_grid:\n",
    "        print(f\"Ajustando hiperparámetros para {model_name}...\")\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"f1\", n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train.values.ravel())\n",
    "        return grid_search.best_estimator_, grid_search.best_params_\n",
    "    else:\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune_models(X_train, y_train, X_test, y_test, models_with_params):\n",
    "    \"\"\"Entrena y ajusta los modelos, guarda sus resultados.\"\"\"\n",
    "    results = {}\n",
    "    print(\"\\nEntrenando modelos y evaluando...\")\n",
    "    for model_name, model in models_with_params.items():\n",
    "        print(f\"\\nEntrenando modelo: {model_name}\")\n",
    "        param_grid = get_hyperparameter_grid(model_name)\n",
    "        best_model, best_params = train_model_with_hyperparameters(model_name, model, param_grid, X_train, y_train)\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"F1-Score para {model_name}: {f1}\")\n",
    "        \n",
    "        # Guardar modelo\n",
    "        model_file = os.path.join(artifacts_path, f\"{model_name.replace(' ', '_').lower()}.pkl\")\n",
    "        with open(model_file, \"wb\") as f:\n",
    "            pickle.dump(best_model, f)\n",
    "\n",
    "        # Guardar resultados\n",
    "        results[model_name] = {\n",
    "            \"f1_score\": f1,\n",
    "            \"hyperparameters\": best_params,\n",
    "            \"model_path\": model_file\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results):\n",
    "    \"\"\"Guarda los resultados en un archivo JSON.\"\"\"\n",
    "    with open(results_file_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(f\"\\nResultados guardados en {results_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(results):\n",
    "    \"\"\"Selecciona el mejor modelo basado en el F1-Score.\"\"\"\n",
    "    best_model_name = max(results, key=lambda x: results[x][\"f1_score\"])\n",
    "    best_model_path = results[best_model_name][\"model_path\"]\n",
    "    print(f\"\\nMejor modelo: {best_model_name} con F1-Score: {results[best_model_name]['f1_score']}\")\n",
    "    return best_model_name, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline():\n",
    "    \"\"\"Carga el pipeline base.\"\"\"\n",
    "    print(\"\\nCargando pipeline base...\")\n",
    "    with open(pipeline_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pipeline_with_model(pipeline, X_train, best_model):\n",
    "    \"\"\"Actualiza el pipeline con el modelo ganador.\"\"\"\n",
    "    def validate_pipeline_columns(pipeline, X):\n",
    "        updated_steps = []\n",
    "        for name, step in pipeline.steps:\n",
    "            if hasattr(step, \"features_to_drop\"):\n",
    "                valid_features = [col for col in step.features_to_drop if col in X.columns]\n",
    "                if valid_features:\n",
    "                    step.features_to_drop = valid_features\n",
    "                    updated_steps.append((name, step))\n",
    "                else:\n",
    "                    print(f\"El paso '{name}' fue eliminado porque no tiene columnas válidas.\")\n",
    "            else:\n",
    "                updated_steps.append((name, step))\n",
    "        return Pipeline(updated_steps)\n",
    "\n",
    "    pipeline = validate_pipeline_columns(pipeline, X_train)\n",
    "    pipeline.steps.append((\"best_model\", best_model))\n",
    "    print(\"Modelo ganador agregado al pipeline.\")\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline(pipeline):\n",
    "    \"\"\"Guarda el pipeline ajustado.\"\"\"\n",
    "    print(f\"\\nGuardando el pipeline ajustado en: {final_pipeline_path}\")\n",
    "    with open(final_pipeline_path, \"wb\") as f:\n",
    "        pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_test, y_test):\n",
    "    \"\"\"Evalúa el pipeline ajustado.\"\"\"\n",
    "    print(\"\\nEvaluando el pipeline ajustado...\")\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"\\nF1-Score del pipeline ajustado: {f1}\")\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función Principal\n",
    "def main():\n",
    "    # Cargar datos\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "    \n",
    "    # Definir modelos\n",
    "    models_with_params = define_models()\n",
    "    \n",
    "    # Entrenar modelos y guardar resultados\n",
    "    results = train_and_tune_models(X_train, y_train, X_test, y_test, models_with_params)\n",
    "    save_results(results)\n",
    "    \n",
    "    # Seleccionar el mejor modelo\n",
    "    best_model_name, best_model_path = select_best_model(results)\n",
    "    \n",
    "    # Cargar el modelo ganador\n",
    "    with open(best_model_path, \"rb\") as f:\n",
    "        best_model = pickle.load(f)\n",
    "    \n",
    "    # Cargar y actualizar pipeline\n",
    "    pipeline = load_pipeline()\n",
    "    pipeline = update_pipeline_with_model(pipeline, X_train, best_model)\n",
    "    \n",
    "    # Ajustar pipeline\n",
    "    print(\"\\nAjustando el pipeline completo...\")\n",
    "    pipeline.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    # Guardar pipeline y evaluar\n",
    "    save_pipeline(pipeline)\n",
    "    evaluate_pipeline(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando datos procesados...\n",
      "\n",
      "Entrenando modelos y evaluando...\n",
      "\n",
      "Entrenando modelo: Logistic Regression\n",
      "Ajustando hiperparámetros para Logistic Regression...\n",
      "F1-Score para Logistic Regression: 0.9175257731958762\n",
      "\n",
      "Entrenando modelo: Decision Tree\n",
      "Ajustando hiperparámetros para Decision Tree...\n",
      "F1-Score para Decision Tree: 0.9183673469387755\n",
      "\n",
      "Entrenando modelo: Random Forest\n",
      "Ajustando hiperparámetros para Random Forest...\n",
      "F1-Score para Random Forest: 0.9166666666666666\n",
      "\n",
      "Entrenando modelo: SVC\n",
      "Ajustando hiperparámetros para SVC...\n",
      "F1-Score para SVC: 0.93\n",
      "\n",
      "Entrenando modelo: XGBoost\n",
      "Ajustando hiperparámetros para XGBoost...\n",
      "F1-Score para XGBoost: 0.9270833333333334\n",
      "\n",
      "Resultados guardados en c:\\Users\\hp i7\\Documents\\Mini_Proyecto_2\\notebooks\\..\\artifacts\\model_results.json\n",
      "\n",
      "Mejor modelo: SVC con F1-Score: 0.93\n",
      "\n",
      "Cargando pipeline base...\n",
      "El paso 'delete_features' fue eliminado porque no tiene columnas válidas.\n",
      "Modelo ganador agregado al pipeline.\n",
      "\n",
      "Ajustando el pipeline completo...\n",
      "\n",
      "Guardando el pipeline ajustado en: c:\\Users\\hp i7\\Documents\\Mini_Proyecto_2\\notebooks\\..\\artifacts\\best_model_pipeline.pkl\n",
      "\n",
      "Evaluando el pipeline ajustado...\n",
      "\n",
      "F1-Score del pipeline ajustado: 0.93\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       197\n",
      "           1       0.92      0.94      0.93        99\n",
      "\n",
      "    accuracy                           0.95       296\n",
      "   macro avg       0.95      0.95      0.95       296\n",
      "weighted avg       0.95      0.95      0.95       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_product_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
