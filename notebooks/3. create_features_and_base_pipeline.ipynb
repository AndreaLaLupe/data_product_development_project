{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import configparser\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.selection import DropFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial\n",
    "project_path = os.getcwd()\n",
    "raw_data_path = os.path.join(project_path, \"..\", \"data\", \"interim\", \"creditcard_balanced.csv\")\n",
    "artifacts_path = os.path.join(project_path, \"..\", \"artifacts\")\n",
    "processed_data_path = os.path.join(project_path, \"..\", \"data\", \"processed\")\n",
    "config_path = os.path.join(project_path, \"..\", \"pipeline.cfg\")\n",
    "\n",
    "# Crear carpeta de artefactos si no existe\n",
    "os.makedirs(artifacts_path, exist_ok=True)\n",
    "os.makedirs(processed_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde c:\\Users\\hp i7\\OneDrive - Universidad Rafael Landivar\\Universidad\\Galileo\\2024\\Cuarto Trimestre\\Product Development\\mini_proyecto_2\\mini_proyecto_3\\Mini_Proyecto_2\\notebooks\\..\\data\\interim\\creditcard_balanced.csv. Dimensiones: (1476, 31)\n"
     ]
    }
   ],
   "source": [
    "# Leer el dataset balanceado\n",
    "try:\n",
    "    data = pd.read_csv(raw_data_path)\n",
    "    print(f\"Datos cargados desde {raw_data_path}. Dimensiones: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"El archivo {raw_data_path} no existe. Verifica la ruta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar archivo de configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\hp i7\\\\OneDrive - Universidad Rafael Landivar\\\\Universidad\\\\Galileo\\\\2024\\\\Cuarto Trimestre\\\\Product Development\\\\mini_proyecto_2\\\\mini_proyecto_3\\\\Mini_Proyecto_2\\\\notebooks\\\\..\\\\pipeline.cfg']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leer configuraciones desde el archivo `pipeline.cfg`\n",
    "config = configparser.ConfigParser()\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"El archivo de configuración {config_path} no existe. Por favor, crea el archivo antes de continuar.\")\n",
    "\n",
    "config.read(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar secciones requeridas\n",
    "required_sections = ['GENERAL', 'CONTINUES', 'CATEGORICAL']\n",
    "missing_sections = [section for section in required_sections if section not in config]\n",
    "if missing_sections:\n",
    "    raise ValueError(f\"Faltan las siguientes secciones en el archivo de configuración: {missing_sections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar columnas según configuración\n",
    "def validate_columns(data, config, section, key):\n",
    "    \"\"\"\n",
    "    Valida que las columnas especificadas en la configuración existan en el dataset.\n",
    "    Maneja casos en los que las secciones están vacías.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): El dataset a validar.\n",
    "        config (ConfigParser): Archivo de configuración cargado.\n",
    "        section (str): Nombre de la sección en el archivo de configuración.\n",
    "        key (str): Clave de la sección a validar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de columnas validadas o vacía si no hay columnas especificadas.\n",
    "    \"\"\"\n",
    "    if key not in config[section] or not config[section][key].strip():\n",
    "        print(f\"No se encontraron columnas para {section} -> {key}.\")\n",
    "        return []\n",
    "    \n",
    "    columns = [col.strip() for col in config[section][key].split(',')]\n",
    "    missing_columns = [col for col in columns if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Las siguientes columnas no existen en el dataset: {missing_columns}\")\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron columnas para CONTINUES -> VARS_TO_IMPUTE.\n",
      "No se encontraron columnas para CATEGORICAL -> VARS_TO_IMPUTE.\n",
      "No se encontraron columnas para CATEGORICAL -> OHE_VARS.\n",
      "No se encontraron columnas para CATEGORICAL -> FREQUENCY_ENC_VARS.\n"
     ]
    }
   ],
   "source": [
    "# Validar columnas según configuración\n",
    "vars_to_drop = validate_columns(data, config, 'GENERAL', 'VARS_TO_DROP')\n",
    "vars_to_impute_continues = validate_columns(data, config, 'CONTINUES', 'VARS_TO_IMPUTE')\n",
    "vars_to_impute_categorical = validate_columns(data, config, 'CATEGORICAL', 'VARS_TO_IMPUTE')\n",
    "ohe_vars = validate_columns(data, config, 'CATEGORICAL', 'OHE_VARS')\n",
    "freq_enc_vars = validate_columns(data, config, 'CATEGORICAL', 'FREQUENCY_ENC_VARS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline creado dinámicamente con los siguientes pasos:\n",
      "- delete_features: DropFeatures\n",
      "- scaling: StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# Crear pipeline dinámico\n",
    "pipeline_steps = []\n",
    "\n",
    "# Paso para eliminar columnas no relevantes\n",
    "if vars_to_drop:\n",
    "    pipeline_steps.append(('delete_features', DropFeatures(features_to_drop=vars_to_drop)))\n",
    "\n",
    "# Paso para imputación de valores continuos\n",
    "if vars_to_impute_continues:\n",
    "    pipeline_steps.append(('mean_imputer', MeanMedianImputer(imputation_method='mean', variables=vars_to_impute_continues)))\n",
    "\n",
    "# Paso para imputación de valores categóricos\n",
    "if vars_to_impute_categorical:\n",
    "    pipeline_steps.append(('categorical_imputer', CategoricalImputer(imputation_method='frequent', variables=vars_to_impute_categorical)))\n",
    "\n",
    "# Paso para codificación One-Hot\n",
    "if ohe_vars:\n",
    "    pipeline_steps.append(('one_hot_encoder', OneHotEncoder(variables=ohe_vars, drop_last=True)))\n",
    "\n",
    "# Paso para codificación por frecuencia\n",
    "if freq_enc_vars:\n",
    "    pipeline_steps.append(('frequency_encoder', CountFrequencyEncoder(encoding_method='count', variables=freq_enc_vars)))\n",
    "\n",
    "# Paso para escalar características\n",
    "pipeline_steps.append(('scaling', StandardScaler()))\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "print(\"\\nPipeline creado dinámicamente con los siguientes pasos:\")\n",
    "for step_name, step in pipeline.steps:\n",
    "    print(f\"- {step_name}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline guardado como archivo: c:\\Users\\hp i7\\OneDrive - Universidad Rafael Landivar\\Universidad\\Galileo\\2024\\Cuarto Trimestre\\Product Development\\mini_proyecto_2\\mini_proyecto_3\\Mini_Proyecto_2\\notebooks\\..\\artifacts\\base_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Guardar el pipeline en la carpeta `artifacts`\n",
    "pipeline_path = os.path.join(artifacts_path, \"base_pipeline.pkl\")\n",
    "try:\n",
    "    with open(pipeline_path, \"wb\") as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "    print(f\"\\nPipeline guardado como archivo: {pipeline_path}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error al guardar el pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjuntos de datos procesados guardados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Ajustar pipeline y generar datos procesados\n",
    "try:\n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X = data.drop(columns=[config['GENERAL']['TARGET']])\n",
    "    y = data[config['GENERAL']['TARGET']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Ajustar el pipeline con los datos de entrenamiento\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Transformar los conjuntos de entrenamiento y prueba\n",
    "    X_train_processed = pipeline.transform(X_train)\n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "\n",
    "    # Guardar los datos procesados\n",
    "    pd.DataFrame(X_train_processed).to_csv(os.path.join(processed_data_path, \"X_train.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(processed_data_path, \"y_train.csv\"), index=False)\n",
    "    pd.DataFrame(X_test_processed).to_csv(os.path.join(processed_data_path, \"X_test.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(processed_data_path, \"y_test.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nConjuntos de datos procesados guardados correctamente.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error al procesar los datos: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_product_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
